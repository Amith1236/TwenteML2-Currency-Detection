{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0222bec8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\vicso\\\\anaconda3\\\\envs\\\\myenv\\\\python.exe'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b082f7e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.9.1-cp311-cp311-win_amd64.whl.metadata (30 kB)\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.24.1-cp311-cp311-win_amd64.whl.metadata (5.9 kB)\n",
      "Collecting torchaudio\n",
      "  Downloading torchaudio-2.9.1-cp311-cp311-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting filelock (from torch)\n",
      "  Downloading filelock-3.20.1-py3-none-any.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\vicso\\anaconda3\\envs\\myenv\\lib\\site-packages (from torch) (4.12.2)\n",
      "Collecting sympy>=1.13.3 (from torch)\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\vicso\\anaconda3\\envs\\myenv\\lib\\site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\vicso\\anaconda3\\envs\\myenv\\lib\\site-packages (from torch) (3.1.6)\n",
      "Collecting fsspec>=0.8.5 (from torch)\n",
      "  Downloading fsspec-2025.12.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\vicso\\anaconda3\\envs\\myenv\\lib\\site-packages (from torchvision) (2.2.5)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\vicso\\anaconda3\\envs\\myenv\\lib\\site-packages (from torchvision) (11.1.0)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\vicso\\anaconda3\\envs\\myenv\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Downloading torch-2.9.1-cp311-cp311-win_amd64.whl (111.0 MB)\n",
      "   ---------------------------------------- 0.0/111.0 MB ? eta -:--:--\n",
      "   - -------------------------------------- 3.4/111.0 MB 18.3 MB/s eta 0:00:06\n",
      "   -- ------------------------------------- 7.1/111.0 MB 17.4 MB/s eta 0:00:06\n",
      "   --- ------------------------------------ 11.0/111.0 MB 17.6 MB/s eta 0:00:06\n",
      "   ----- ---------------------------------- 14.7/111.0 MB 17.8 MB/s eta 0:00:06\n",
      "   ------ --------------------------------- 18.1/111.0 MB 17.3 MB/s eta 0:00:06\n",
      "   ------- -------------------------------- 22.0/111.0 MB 17.4 MB/s eta 0:00:06\n",
      "   --------- ------------------------------ 25.7/111.0 MB 17.3 MB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 29.1/111.0 MB 17.4 MB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 32.8/111.0 MB 17.3 MB/s eta 0:00:05\n",
      "   ------------- -------------------------- 36.4/111.0 MB 17.4 MB/s eta 0:00:05\n",
      "   -------------- ------------------------- 40.4/111.0 MB 17.3 MB/s eta 0:00:05\n",
      "   --------------- ------------------------ 44.0/111.0 MB 17.4 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 47.7/111.0 MB 17.5 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 51.4/111.0 MB 17.4 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 55.1/111.0 MB 17.4 MB/s eta 0:00:04\n",
      "   --------------------- ------------------ 59.0/111.0 MB 17.4 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 62.9/111.0 MB 17.4 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 64.7/111.0 MB 16.9 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 67.6/111.0 MB 16.7 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 71.3/111.0 MB 16.8 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 75.0/111.0 MB 16.8 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 78.6/111.0 MB 16.8 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 82.3/111.0 MB 16.8 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 85.2/111.0 MB 16.7 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 88.9/111.0 MB 16.7 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 92.5/111.0 MB 16.7 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 96.2/111.0 MB 16.7 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 99.9/111.0 MB 16.8 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 103.5/111.0 MB 16.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 107.2/111.0 MB 16.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  110.9/111.0 MB 16.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  110.9/111.0 MB 16.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  110.9/111.0 MB 16.9 MB/s eta 0:00:01\n",
      "   --------------------------------------- 111.0/111.0 MB 15.7 MB/s eta 0:00:00\n",
      "Downloading torchvision-0.24.1-cp311-cp311-win_amd64.whl (4.0 MB)\n",
      "   ---------------------------------------- 0.0/4.0 MB ? eta -:--:--\n",
      "   ------------------------------------ --- 3.7/4.0 MB 18.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 4.0/4.0 MB 14.2 MB/s eta 0:00:00\n",
      "Downloading torchaudio-2.9.1-cp311-cp311-win_amd64.whl (664 kB)\n",
      "   ---------------------------------------- 0.0/664.7 kB ? eta -:--:--\n",
      "   --------------------------------------- 664.7/664.7 kB 13.0 MB/s eta 0:00:00\n",
      "Downloading fsspec-2025.12.0-py3-none-any.whl (201 kB)\n",
      "Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "   ---------------------------------------- 0.0/6.3 MB ? eta -:--:--\n",
      "   ----------------------- ---------------- 3.7/6.3 MB 18.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.3/6.3 MB 14.8 MB/s eta 0:00:00\n",
      "Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "   ---------------------------------------- 0.0/536.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 536.2/536.2 kB 8.8 MB/s eta 0:00:00\n",
      "Downloading filelock-3.20.1-py3-none-any.whl (16 kB)\n",
      "Installing collected packages: mpmath, sympy, fsspec, filelock, torch, torchvision, torchaudio\n",
      "\n",
      "   ---------------------------------------- 0/7 [mpmath]\n",
      "   ---------------------------------------- 0/7 [mpmath]\n",
      "   ---------------------------------------- 0/7 [mpmath]\n",
      "   ---------------------------------------- 0/7 [mpmath]\n",
      "   ---------------------------------------- 0/7 [mpmath]\n",
      "   ---------------------------------------- 0/7 [mpmath]\n",
      "   ---------------------------------------- 0/7 [mpmath]\n",
      "   ----- ---------------------------------- 1/7 [sympy]\n",
      "   ----- ---------------------------------- 1/7 [sympy]\n",
      "   ----- ---------------------------------- 1/7 [sympy]\n",
      "   ----- ---------------------------------- 1/7 [sympy]\n",
      "   ----- ---------------------------------- 1/7 [sympy]\n",
      "   ----- ---------------------------------- 1/7 [sympy]\n",
      "   ----- ---------------------------------- 1/7 [sympy]\n",
      "   ----- ---------------------------------- 1/7 [sympy]\n",
      "   ----- ---------------------------------- 1/7 [sympy]\n",
      "   ----- ---------------------------------- 1/7 [sympy]\n",
      "   ----- ---------------------------------- 1/7 [sympy]\n",
      "   ----- ---------------------------------- 1/7 [sympy]\n",
      "   ----- ---------------------------------- 1/7 [sympy]\n",
      "   ----- ---------------------------------- 1/7 [sympy]\n",
      "   ----- ---------------------------------- 1/7 [sympy]\n",
      "   ----- ---------------------------------- 1/7 [sympy]\n",
      "   ----- ---------------------------------- 1/7 [sympy]\n",
      "   ----- ---------------------------------- 1/7 [sympy]\n",
      "   ----- ---------------------------------- 1/7 [sympy]\n",
      "   ----- ---------------------------------- 1/7 [sympy]\n",
      "   ----- ---------------------------------- 1/7 [sympy]\n",
      "   ----- ---------------------------------- 1/7 [sympy]\n",
      "   ----- ---------------------------------- 1/7 [sympy]\n",
      "   ----- ---------------------------------- 1/7 [sympy]\n",
      "   ----- ---------------------------------- 1/7 [sympy]\n",
      "   ----- ---------------------------------- 1/7 [sympy]\n",
      "   ----- ---------------------------------- 1/7 [sympy]\n",
      "   ----- ---------------------------------- 1/7 [sympy]\n",
      "   ----- ---------------------------------- 1/7 [sympy]\n",
      "   ----- ---------------------------------- 1/7 [sympy]\n",
      "   ----- ---------------------------------- 1/7 [sympy]\n",
      "   ----- ---------------------------------- 1/7 [sympy]\n",
      "   ----- ---------------------------------- 1/7 [sympy]\n",
      "   ----- ---------------------------------- 1/7 [sympy]\n",
      "   ----- ---------------------------------- 1/7 [sympy]\n",
      "   ----- ---------------------------------- 1/7 [sympy]\n",
      "   ----- ---------------------------------- 1/7 [sympy]\n",
      "   ----- ---------------------------------- 1/7 [sympy]\n",
      "   ----- ---------------------------------- 1/7 [sympy]\n",
      "   ----- ---------------------------------- 1/7 [sympy]\n",
      "   ----- ---------------------------------- 1/7 [sympy]\n",
      "   ----- ---------------------------------- 1/7 [sympy]\n",
      "   ----- ---------------------------------- 1/7 [sympy]\n",
      "   ----- ---------------------------------- 1/7 [sympy]\n",
      "   ----- ---------------------------------- 1/7 [sympy]\n",
      "   ----- ---------------------------------- 1/7 [sympy]\n",
      "   ----- ---------------------------------- 1/7 [sympy]\n",
      "   ----- ---------------------------------- 1/7 [sympy]\n",
      "   ----- ---------------------------------- 1/7 [sympy]\n",
      "   ----- ---------------------------------- 1/7 [sympy]\n",
      "   ----- ---------------------------------- 1/7 [sympy]\n",
      "   ----- ---------------------------------- 1/7 [sympy]\n",
      "   ----- ---------------------------------- 1/7 [sympy]\n",
      "   ----- ---------------------------------- 1/7 [sympy]\n",
      "   ----- ---------------------------------- 1/7 [sympy]\n",
      "   ----- ---------------------------------- 1/7 [sympy]\n",
      "   ----- ---------------------------------- 1/7 [sympy]\n",
      "   ----- ---------------------------------- 1/7 [sympy]\n",
      "   ----- ---------------------------------- 1/7 [sympy]\n",
      "   ----- ---------------------------------- 1/7 [sympy]\n",
      "   ----- ---------------------------------- 1/7 [sympy]\n",
      "   ----- ---------------------------------- 1/7 [sympy]\n",
      "   ----- ---------------------------------- 1/7 [sympy]\n",
      "   ----- ---------------------------------- 1/7 [sympy]\n",
      "   ----- ---------------------------------- 1/7 [sympy]\n",
      "   ----------- ---------------------------- 2/7 [fsspec]\n",
      "   ----------- ---------------------------- 2/7 [fsspec]\n",
      "   ---------------------- ----------------- 4/7 [torch]\n",
      "   ---------------------- ----------------- 4/7 [torch]\n",
      "   ---------------------- ----------------- 4/7 [torch]\n",
      "   ---------------------- ----------------- 4/7 [torch]\n",
      "   ---------------------- ----------------- 4/7 [torch]\n",
      "   ---------------------- ----------------- 4/7 [torch]\n",
      "   ---------------------- ----------------- 4/7 [torch]\n",
      "   ---------------------- ----------------- 4/7 [torch]\n",
      "   ---------------------- ----------------- 4/7 [torch]\n",
      "   ---------------------- ----------------- 4/7 [torch]\n",
      "   ---------------------- ----------------- 4/7 [torch]\n",
      "   ---------------------- ----------------- 4/7 [torch]\n",
      "   ---------------------- ----------------- 4/7 [torch]\n",
      "   ---------------------- ----------------- 4/7 [torch]\n",
      "   ---------------------- ----------------- 4/7 [torch]\n",
      "   ---------------------- ----------------- 4/7 [torch]\n",
      "   ---------------------- ----------------- 4/7 [torch]\n",
      "   ---------------------- ----------------- 4/7 [torch]\n",
      "   ---------------------- ----------------- 4/7 [torch]\n",
      "   ---------------------- ----------------- 4/7 [torch]\n",
      "   ---------------------- ----------------- 4/7 [torch]\n",
      "   ---------------------- ----------------- 4/7 [torch]\n",
      "   ---------------------- ----------------- 4/7 [torch]\n",
      "   ---------------------- ----------------- 4/7 [torch]\n",
      "   ---------------------- ----------------- 4/7 [torch]\n",
      "   ---------------------- ----------------- 4/7 [torch]\n",
      "   ---------------------- ----------------- 4/7 [torch]\n",
      "   ---------------------- ----------------- 4/7 [torch]\n",
      "   ---------------------- ----------------- 4/7 [torch]\n",
      "   ---------------------- ----------------- 4/7 [torch]\n",
      "   ---------------------- ----------------- 4/7 [torch]\n",
      "   ---------------------- ----------------- 4/7 [torch]\n",
      "   ---------------------- ----------------- 4/7 [torch]\n",
      "   ---------------------- ----------------- 4/7 [torch]\n",
      "   ---------------------- ----------------- 4/7 [torch]\n",
      "   ---------------------- ----------------- 4/7 [torch]\n",
      "   ---------------------- ----------------- 4/7 [torch]\n",
      "   ---------------------- ----------------- 4/7 [torch]\n",
      "   ---------------------- ----------------- 4/7 [torch]\n",
      "   ---------------------- ----------------- 4/7 [torch]\n",
      "   ---------------------- ----------------- 4/7 [torch]\n",
      "   ---------------------- ----------------- 4/7 [torch]\n",
      "   ---------------------- ----------------- 4/7 [torch]\n",
      "   ---------------------- ----------------- 4/7 [torch]\n",
      "   ---------------------- ----------------- 4/7 [torch]\n",
      "   ---------------------- ----------------- 4/7 [torch]\n",
      "   ---------------------- ----------------- 4/7 [torch]\n",
      "   ---------------------- ----------------- 4/7 [torch]\n",
      "   ---------------------- ----------------- 4/7 [torch]\n",
      "   ---------------------- ----------------- 4/7 [torch]\n",
      "   ---------------------- ----------------- 4/7 [torch]\n",
      "   ---------------------- ----------------- 4/7 [torch]\n",
      "   ---------------------- ----------------- 4/7 [torch]\n",
      "   ---------------------- ----------------- 4/7 [torch]\n",
      "   ---------------------- ----------------- 4/7 [torch]\n",
      "   ---------------------- ----------------- 4/7 [torch]\n",
      "   ---------------------- ----------------- 4/7 [torch]\n",
      "   ---------------------- ----------------- 4/7 [torch]\n",
      "   ---------------------- ----------------- 4/7 [torch]\n",
      "   ---------------------- ----------------- 4/7 [torch]\n",
      "   ---------------------- ----------------- 4/7 [torch]\n",
      "   ---------------------- ----------------- 4/7 [torch]\n",
      "   ---------------------- ----------------- 4/7 [torch]\n",
      "   ---------------------- ----------------- 4/7 [torch]\n",
      "   ---------------------- ----------------- 4/7 [torch]\n",
      "   ---------------------- ----------------- 4/7 [torch]\n",
      "   ---------------------- ----------------- 4/7 [torch]\n",
      "   ---------------------- ----------------- 4/7 [torch]\n",
      "   ---------------------- ----------------- 4/7 [torch]\n",
      "   ---------------------- ----------------- 4/7 [torch]\n",
      "   ---------------------- ----------------- 4/7 [torch]\n",
      "   ---------------------- ----------------- 4/7 [torch]\n",
      "   ---------------------- ----------------- 4/7 [torch]\n",
      "   ---------------------- ----------------- 4/7 [torch]\n",
      "   ---------------------- ----------------- 4/7 [torch]\n",
      "   ---------------------- ----------------- 4/7 [torch]\n",
      "   ---------------------- ----------------- 4/7 [torch]\n",
      "   ---------------------- ----------------- 4/7 [torch]\n",
      "   ---------------------- ----------------- 4/7 [torch]\n",
      "   ---------------------- ----------------- 4/7 [torch]\n",
      "   ---------------------- ----------------- 4/7 [torch]\n",
      "   ---------------------- ----------------- 4/7 [torch]\n",
      "   ---------------------- ----------------- 4/7 [torch]\n",
      "   ---------------------- ----------------- 4/7 [torch]\n",
      "   ---------------------- ----------------- 4/7 [torch]\n",
      "   ---------------------- ----------------- 4/7 [torch]\n",
      "   ---------------------- ----------------- 4/7 [torch]\n",
      "   ---------------------- ----------------- 4/7 [torch]\n",
      "   ---------------------- ----------------- 4/7 [torch]\n",
      "   ---------------------- ----------------- 4/7 [torch]\n",
      "   ---------------------- ----------------- 4/7 [torch]\n",
      "   ---------------------- ----------------- 4/7 [torch]\n",
      "   ---------------------- ----------------- 4/7 [torch]\n",
      "   ---------------------- ----------------- 4/7 [torch]\n",
      "   ---------------------- ----------------- 4/7 [torch]\n",
      "   ---------------------- ----------------- 4/7 [torch]\n",
      "   ---------------------- ----------------- 4/7 [torch]\n",
      "   ---------------------- ----------------- 4/7 [torch]\n",
      "   ---------------------- ----------------- 4/7 [torch]\n",
      "   ---------------------- ----------------- 4/7 [torch]\n",
      "   ---------------------- ----------------- 4/7 [torch]\n",
      "   ---------------------- ----------------- 4/7 [torch]\n",
      "   ---------------------- ----------------- 4/7 [torch]\n",
      "   ---------------------- ----------------- 4/7 [torch]\n",
      "   ---------------------- ----------------- 4/7 [torch]\n",
      "   ---------------------- ----------------- 4/7 [torch]\n",
      "   ---------------------- ----------------- 4/7 [torch]\n",
      "   ---------------------- ----------------- 4/7 [torch]\n",
      "   ---------------------- ----------------- 4/7 [torch]\n",
      "   ---------------------- ----------------- 4/7 [torch]\n",
      "   ---------------------- ----------------- 4/7 [torch]\n",
      "   ---------------------- ----------------- 4/7 [torch]\n",
      "   ---------------------- ----------------- 4/7 [torch]\n",
      "   ---------------------- ----------------- 4/7 [torch]\n",
      "   ---------------------- ----------------- 4/7 [torch]\n",
      "   ---------------------- ----------------- 4/7 [torch]\n",
      "   ---------------------- ----------------- 4/7 [torch]\n",
      "   ---------------------- ----------------- 4/7 [torch]\n",
      "   ---------------------- ----------------- 4/7 [torch]\n",
      "   ---------------------- ----------------- 4/7 [torch]\n",
      "   ---------------------- ----------------- 4/7 [torch]\n",
      "   ---------------------- ----------------- 4/7 [torch]\n",
      "   ---------------------- ----------------- 4/7 [torch]\n",
      "   ---------------------- ----------------- 4/7 [torch]\n",
      "   ---------------------- ----------------- 4/7 [torch]\n",
      "   ---------------------- ----------------- 4/7 [torch]\n",
      "   ---------------------- ----------------- 4/7 [torch]\n",
      "   ---------------------- ----------------- 4/7 [torch]\n",
      "   ---------------------- ----------------- 4/7 [torch]\n",
      "   ---------------------- ----------------- 4/7 [torch]\n",
      "   ---------------------- ----------------- 4/7 [torch]\n",
      "   ---------------------- ----------------- 4/7 [torch]\n",
      "   ---------------------- ----------------- 4/7 [torch]\n",
      "   ---------------------- ----------------- 4/7 [torch]\n",
      "   ---------------------- ----------------- 4/7 [torch]\n",
      "   ---------------------- ----------------- 4/7 [torch]\n",
      "   ---------------------- ----------------- 4/7 [torch]\n",
      "   ---------------------- ----------------- 4/7 [torch]\n",
      "   ---------------------- ----------------- 4/7 [torch]\n",
      "   ---------------------- ----------------- 4/7 [torch]\n",
      "   ---------------------- ----------------- 4/7 [torch]\n",
      "   ---------------------- ----------------- 4/7 [torch]\n",
      "   ---------------------------- ----------- 5/7 [torchvision]\n",
      "   ---------------------------- ----------- 5/7 [torchvision]\n",
      "   ---------------------------- ----------- 5/7 [torchvision]\n",
      "   ---------------------------- ----------- 5/7 [torchvision]\n",
      "   ---------------------------- ----------- 5/7 [torchvision]\n",
      "   ---------------------------- ----------- 5/7 [torchvision]\n",
      "   ---------------------------------- ----- 6/7 [torchaudio]\n",
      "   ---------------------------------- ----- 6/7 [torchaudio]\n",
      "   ---------------------------------------- 7/7 [torchaudio]\n",
      "\n",
      "Successfully installed filelock-3.20.1 fsspec-2025.12.0 mpmath-1.3.0 sympy-1.14.0 torch-2.9.1 torchaudio-2.9.1 torchvision-0.24.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script isympy.exe is installed in 'C:\\Users\\vicso\\anaconda3\\envs\\myenv\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The scripts torchfrtrace.exe and torchrun.exe are installed in 'C:\\Users\\vicso\\anaconda3\\envs\\myenv\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50ff49d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in c:\\users\\vicso\\anaconda3\\envs\\myenv\\lib\\site-packages (3.10.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\vicso\\anaconda3\\envs\\myenv\\lib\\site-packages (1.7.0)\n",
      "Requirement already satisfied: pillow in c:\\users\\vicso\\anaconda3\\envs\\myenv\\lib\\site-packages (11.1.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\vicso\\anaconda3\\envs\\myenv\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\vicso\\anaconda3\\envs\\myenv\\lib\\site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\vicso\\anaconda3\\envs\\myenv\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\vicso\\anaconda3\\envs\\myenv\\lib\\site-packages (from matplotlib) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\vicso\\anaconda3\\envs\\myenv\\lib\\site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\users\\vicso\\anaconda3\\envs\\myenv\\lib\\site-packages (from matplotlib) (2.2.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\vicso\\anaconda3\\envs\\myenv\\lib\\site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\vicso\\anaconda3\\envs\\myenv\\lib\\site-packages (from matplotlib) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\vicso\\anaconda3\\envs\\myenv\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: scipy>=1.8.0 in c:\\users\\vicso\\anaconda3\\envs\\myenv\\lib\\site-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\vicso\\anaconda3\\envs\\myenv\\lib\\site-packages (from scikit-learn) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\vicso\\anaconda3\\envs\\myenv\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\vicso\\anaconda3\\envs\\myenv\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\vicso\\anaconda3\\envs\\myenv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!{sys.executable} -m pip install matplotlib scikit-learn pillow tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8976abd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All imports successful ✅\n"
     ]
    }
   ],
   "source": [
    "import torch, torchvision\n",
    "import matplotlib\n",
    "import sklearn\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"All imports successful ✅\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6265e6dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes:\n",
      "THAI100: 320\n",
      "THAI1000: 320\n",
      "THAI20: 320\n",
      "THAI50: 320\n",
      "THAI500: 320\n",
      "\n",
      "Total images: 1600\n"
     ]
    }
   ],
   "source": [
    "import os, glob\n",
    "from collections import Counter\n",
    "#tells me whats inside my dataset\n",
    "\n",
    "\n",
    "DATA_DIR = r\"C:\\Users\\vicso\\Downloads\\ThaiBankNotes\\ThaiBankNotes\\Training\"\n",
    "\n",
    "classes = sorted([\n",
    "    d for d in os.listdir(DATA_DIR)\n",
    "    if os.path.isdir(os.path.join(DATA_DIR, d))\n",
    "])\n",
    "\n",
    "counts = {}\n",
    "for c in classes:\n",
    "    folder = os.path.join(DATA_DIR, c)\n",
    "    imgs = (\n",
    "        glob.glob(os.path.join(folder, \"*.jpg\")) +\n",
    "        glob.glob(os.path.join(folder, \"*.JPG\")) +\n",
    "        glob.glob(os.path.join(folder, \"*.png\"))\n",
    "    )\n",
    "    counts[c] = len(imgs)\n",
    "\n",
    "print(\"Classes:\")\n",
    "for k, v in counts.items():\n",
    "    print(f\"{k}: {v}\")\n",
    "\n",
    "print(\"\\nTotal images:\", sum(counts.values()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89e3a83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "BASE_DIR = Path(r\"C:\\Users\\vicso\\Downloads\\ThaiBankNotes\\ThaiBankNotes\")\n",
    "TRAIN_DIR = BASE_DIR / \"Training\"\n",
    "VAL_DIR   = BASE_DIR / \"Validation\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "210c06ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-images in Training: 800\n",
      "Non-images in Validation: 200\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from torchvision.datasets import ImageFolder\n",
    "from PIL import Image\n",
    "\n",
    "IMG_EXTS = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".webp\", \".JPG\", \".JPEG\", \".PNG\"}\n",
    "\n",
    "def is_valid_file(path: str) -> bool:\n",
    "    return os.path.splitext(path)[1] in IMG_EXTS\n",
    "\n",
    "# quick check: how many non-images are present?\n",
    "def count_non_images(root):\n",
    "    n = 0\n",
    "    for dirpath, _, filenames in os.walk(root):\n",
    "        for f in filenames:\n",
    "            if os.path.splitext(f)[1] not in IMG_EXTS:\n",
    "                n += 1\n",
    "    return n\n",
    "\n",
    "print(\"Non-images in Training:\", count_non_images(TRAIN_DIR))\n",
    "print(\"Non-images in Validation:\", count_non_images(VAL_DIR))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6fc717b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['THAI100', 'THAI1000', 'THAI20', 'THAI50', 'THAI500']\n",
      "Train size: 800\n",
      "Val size: 200\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "train_tfms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(p=0.3),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ColorJitter(brightness=0.15, contrast=0.15, saturation=0.15, hue=0.02),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "val_tfms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "train_ds = ImageFolder(root=str(TRAIN_DIR), transform=train_tfms, is_valid_file=is_valid_file)\n",
    "val_ds   = ImageFolder(root=str(VAL_DIR), transform=val_tfms,   is_valid_file=is_valid_file)\n",
    "\n",
    "print(\"Classes:\", train_ds.classes)\n",
    "print(\"Train size:\", len(train_ds))\n",
    "print(\"Val size:\", len(val_ds))\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True, num_workers=0)\n",
    "val_loader   = DataLoader(val_ds, batch_size=32, shuffle=False, num_workers=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "479243b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train split size: 680\n",
      "Test split size: 120\n",
      "Val size (unchanged): 200\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "\n",
    "SEED = 42\n",
    "TEST_FRAC = 0.15\n",
    "\n",
    "targets = np.array(train_ds.targets)  # class index for each sample\n",
    "num_classes = len(train_ds.classes)\n",
    "\n",
    "rng = np.random.default_rng(SEED)\n",
    "\n",
    "train_indices = []\n",
    "test_indices = []\n",
    "\n",
    "for c in range(num_classes):\n",
    "    idx_c = np.where(targets == c)[0]\n",
    "    rng.shuffle(idx_c)\n",
    "\n",
    "    n_test = int(round(len(idx_c) * TEST_FRAC))\n",
    "    test_indices.extend(idx_c[:n_test].tolist())\n",
    "    train_indices.extend(idx_c[n_test:].tolist())\n",
    "\n",
    "# shuffle overall indices (optional)\n",
    "rng.shuffle(train_indices)\n",
    "rng.shuffle(test_indices)\n",
    "\n",
    "train_split = Subset(train_ds, train_indices)\n",
    "test_split  = Subset(train_ds, test_indices)\n",
    "\n",
    "print(\"Train split size:\", len(train_split))\n",
    "print(\"Test split size:\", len(test_split))\n",
    "print(\"Val size (unchanged):\", len(val_ds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c95590c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_split, batch_size=32, shuffle=True, num_workers=0)\n",
    "val_loader   = DataLoader(val_ds,      batch_size=32, shuffle=False, num_workers=0)\n",
    "test_loader  = DataLoader(test_split,  batch_size=32, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a45a438",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m confusion_matrix, classification_report\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m model.eval()\n\u001b[32m      5\u001b[39m all_preds, all_true = [], []\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n",
      "\u001b[31mNameError\u001b[39m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "model.eval()\n",
    "all_preds, all_true = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x, y in test_loader:\n",
    "        x = x.to(DEVICE)\n",
    "        preds = model(x).argmax(1).cpu().numpy()\n",
    "        all_preds.extend(preds)\n",
    "        all_true.extend(y.numpy())\n",
    "\n",
    "cm = confusion_matrix(all_true, all_preds)\n",
    "print(\"Confusion matrix (TEST):\\n\", cm)\n",
    "print(\"\\nReport (TEST):\\n\", classification_report(all_true, all_preds, target_names=train_ds.classes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b627f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to C:\\Users\\vicso/.cache\\torch\\hub\\checkpoints\\resnet50-11ad3fa6.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 97.8M/97.8M [00:06<00:00, 17.0MB/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import models\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", DEVICE)\n",
    "\n",
    "model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "model.fc = nn.Linear(model.fc.in_features, len(train_ds.classes))\n",
    "model = model.to(DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50e3a960",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc9a9371",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train_acc=0.468, val_acc=0.790\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: train_acc=0.962, val_acc=0.940\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: train_acc=0.993, val_acc=0.975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: train_acc=0.997, val_acc=0.970\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: train_acc=0.994, val_acc=0.960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def run_epoch(model, loader, train=True):\n",
    "    model.train() if train else model.eval()\n",
    "    total_loss, total_correct, total = 0.0, 0, 0\n",
    "\n",
    "    for x, y in tqdm(loader, leave=False):\n",
    "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "\n",
    "        if train:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        with torch.set_grad_enabled(train):\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits, y)\n",
    "            if train:\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * x.size(0)\n",
    "        total_correct += (logits.argmax(1) == y).sum().item()\n",
    "        total += x.size(0)\n",
    "\n",
    "    return total_loss / total, total_correct / total\n",
    "\n",
    "for epoch in range(1, 6):\n",
    "    train_loss, train_acc = run_epoch(model, train_loader, train=True)\n",
    "    val_loss, val_acc = run_epoch(model, val_loader, train=False)\n",
    "    print(f\"Epoch {epoch}: train_acc={train_acc:.3f}, val_acc={val_acc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "439618e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train_acc=1.000, val_acc=0.985\n",
      "✅ Saved new best model (val_acc=0.985)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: train_acc=0.997, val_acc=0.980\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: train_acc=0.999, val_acc=0.970\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: train_acc=0.991, val_acc=0.970\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: train_acc=0.996, val_acc=0.980\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "best_val_acc = -1.0\n",
    "BEST_PATH = \"resnet50_thai_rgb_best.pth\"\n",
    "\n",
    "for epoch in range(1, 6):\n",
    "    train_loss, train_acc = run_epoch(model, train_loader, train=True)\n",
    "    val_loss, val_acc = run_epoch(model, val_loader, train=False)\n",
    "    print(f\"Epoch {epoch}: train_acc={train_acc:.3f}, val_acc={val_acc:.3f}\")\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), BEST_PATH)\n",
    "        print(f\"✅ Saved new best model (val_acc={best_val_acc:.3f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a2bf94d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix (TEST):\n",
      " [[24  0  0  0  0]\n",
      " [ 0 24  0  0  0]\n",
      " [ 0  0 24  0  0]\n",
      " [ 0  0  0 24  0]\n",
      " [ 0  1  0  0 23]]\n",
      "\n",
      "Report (TEST):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     THAI100       1.00      1.00      1.00        24\n",
      "    THAI1000       0.96      1.00      0.98        24\n",
      "      THAI20       1.00      1.00      1.00        24\n",
      "      THAI50       1.00      1.00      1.00        24\n",
      "     THAI500       1.00      0.96      0.98        24\n",
      "\n",
      "    accuracy                           0.99       120\n",
      "   macro avg       0.99      0.99      0.99       120\n",
      "weighted avg       0.99      0.99      0.99       120\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Load best checkpoint\n",
    "BEST_PATH = \"resnet50_thai_rgb_best.pth\"\n",
    "model.load_state_dict(torch.load(BEST_PATH, map_location=DEVICE))\n",
    "model.eval()\n",
    "\n",
    "all_preds, all_true = [], []\n",
    "with torch.no_grad():\n",
    "    for x, y in test_loader:\n",
    "        x = x.to(DEVICE)\n",
    "        preds = model(x).argmax(1).cpu().numpy()\n",
    "        all_preds.extend(preds)\n",
    "        all_true.extend(y.numpy())\n",
    "\n",
    "cm = confusion_matrix(all_true, all_preds)\n",
    "print(\"Confusion matrix (TEST):\\n\", cm)\n",
    "print(\"\\nReport (TEST):\\n\", classification_report(all_true, all_preds, target_names=train_ds.classes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1ae7685e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['THAI100', 'THAI1000', 'THAI20', 'THAI50', 'THAI500']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "IMG_EXTS = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".webp\", \".JPG\", \".JPEG\", \".PNG\"}\n",
    "\n",
    "def is_valid_file(path: str) -> bool:\n",
    "    return os.path.splitext(path)[1] in IMG_EXTS\n",
    "\n",
    "# Grayscale transforms:\n",
    "# - Grayscale(num_output_channels=3) keeps 3 channels so ResNet50 still works unchanged.\n",
    "train_tfms_gray = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.RandomHorizontalFlip(p=0.3),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],  # keep ImageNet normalization\n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "val_tfms_gray = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Recreate ImageFolder datasets but with grayscale transforms\n",
    "train_ds_gray = ImageFolder(root=str(TRAIN_DIR), transform=train_tfms_gray, is_valid_file=is_valid_file)\n",
    "val_ds_gray   = ImageFolder(root=str(VAL_DIR),   transform=val_tfms_gray,   is_valid_file=is_valid_file)\n",
    "\n",
    "# Sanity: classes should match exactly\n",
    "print(\"Classes:\", train_ds_gray.classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cbfed81a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gray Train: 680 Gray Val: 200 Gray Test: 120\n"
     ]
    }
   ],
   "source": [
    "train_split_gray = Subset(train_ds_gray, train_indices)\n",
    "test_split_gray  = Subset(train_ds_gray, test_indices)\n",
    "\n",
    "train_loader_gray = DataLoader(train_split_gray, batch_size=32, shuffle=True,  num_workers=0)\n",
    "val_loader_gray   = DataLoader(val_ds_gray,      batch_size=32, shuffle=False, num_workers=0)\n",
    "test_loader_gray  = DataLoader(test_split_gray,  batch_size=32, shuffle=False, num_workers=0)\n",
    "\n",
    "print(\"Gray Train:\", len(train_split_gray), \"Gray Val:\", len(val_ds_gray), \"Gray Test:\", len(test_split_gray))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1035c5ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import models\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", DEVICE)\n",
    "\n",
    "model_gray = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "model_gray.fc = nn.Linear(model_gray.fc.in_features, len(train_ds_gray.classes))\n",
    "model_gray = model_gray.to(DEVICE)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model_gray.parameters(), lr=1e-4, weight_decay=1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ef384235",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GRAY] Epoch 1: train_acc=0.394, val_acc=0.555\n",
      "✅ Saved new best GRAY model (val_acc=0.555)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GRAY] Epoch 2: train_acc=0.801, val_acc=0.765\n",
      "✅ Saved new best GRAY model (val_acc=0.765)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GRAY] Epoch 3: train_acc=0.935, val_acc=0.890\n",
      "✅ Saved new best GRAY model (val_acc=0.890)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GRAY] Epoch 4: train_acc=0.965, val_acc=0.945\n",
      "✅ Saved new best GRAY model (val_acc=0.945)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GRAY] Epoch 5: train_acc=0.979, val_acc=0.955\n",
      "✅ Saved new best GRAY model (val_acc=0.955)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "best_val_acc = -1.0\n",
    "BEST_GRAY_PATH = \"resnet50_thai_gray_best.pth\"\n",
    "\n",
    "for epoch in range(1, 6):\n",
    "    train_loss, train_acc = run_epoch(model_gray, train_loader_gray, train=True)\n",
    "    val_loss, val_acc     = run_epoch(model_gray, val_loader_gray,   train=False)\n",
    "    print(f\"[GRAY] Epoch {epoch}: train_acc={train_acc:.3f}, val_acc={val_acc:.3f}\")\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model_gray.state_dict(), BEST_GRAY_PATH)\n",
    "        print(f\"✅ Saved new best GRAY model (val_acc={best_val_acc:.3f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4706b3d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix (GRAY TEST):\n",
      " [[24  0  0  0  0]\n",
      " [ 0 24  0  0  0]\n",
      " [ 0  0 24  0  0]\n",
      " [ 0  0  0 24  0]\n",
      " [ 0  1  0  0 23]]\n",
      "\n",
      "Report (GRAY TEST):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     THAI100       1.00      1.00      1.00        24\n",
      "    THAI1000       0.96      1.00      0.98        24\n",
      "      THAI20       1.00      1.00      1.00        24\n",
      "      THAI50       1.00      1.00      1.00        24\n",
      "     THAI500       1.00      0.96      0.98        24\n",
      "\n",
      "    accuracy                           0.99       120\n",
      "   macro avg       0.99      0.99      0.99       120\n",
      "weighted avg       0.99      0.99      0.99       120\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "model_gray.load_state_dict(torch.load(BEST_GRAY_PATH, map_location=DEVICE))\n",
    "model_gray.eval()\n",
    "\n",
    "all_preds, all_true = [], []\n",
    "with torch.no_grad():\n",
    "    for x, y in test_loader_gray:\n",
    "        x = x.to(DEVICE)\n",
    "        preds = model_gray(x).argmax(1).cpu().numpy()\n",
    "        all_preds.extend(preds)\n",
    "        all_true.extend(y.numpy())\n",
    "\n",
    "cm = confusion_matrix(all_true, all_preds)\n",
    "print(\"Confusion matrix (GRAY TEST):\\n\", cm)\n",
    "print(\"\\nReport (GRAY TEST):\\n\", classification_report(all_true, all_preds, target_names=train_ds_gray.classes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5156ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
